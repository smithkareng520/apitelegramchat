# -*- coding: utf-8 -*-
import requests
import random
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time
import logging
from typing import List, Dict, Optional
from pathlib import PurePosixPath

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# æœç´¢å¼•æ“é…ç½®ï¼ˆä¸å˜ï¼‰
API_CONFIG = {
    "google": {
        "api_key": "AIzaSyAhnNDFupCb_gn5ZatjGS8xYDHcYpEl5TA",
        "cx": "5485297013e9949eb",
        "search_url": "https://www.googleapis.com/customsearch/v1",
        "enabled": True
    }
}

EXCLUDED_DOMAINS = [
    'csdn.net', 'blog.csdn.net',
    'zhihu.com', 'www.zhihu.com',
    'baidu.com', 'm.baidu.com',
]

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

def _is_excluded_url(url: str) -> bool:
    """æ£€æŸ¥URLæ˜¯å¦å±äºæ’é™¤åˆ—è¡¨"""
    try:
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        return any(excluded in domain for excluded in EXCLUDED_DOMAINS)
    except Exception as e:
        logger.warning(f"URLè§£æå¤±è´¥: {url}, é”™è¯¯: {str(e)}")
        return False

def fetch_webpage_content(url: str, max_retries: int = 3, verify_ssl: bool = True) -> str:
    """æŠ“å–ç½‘é¡µå†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶å’ŒSSLå¤„ç†"""
    headers = {"User-Agent": random.choice(USER_AGENTS)}
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=10, verify=verify_ssl)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            content = _extract_main_content(soup)
            return _clean_content(content, max_length=5000)
        except requests.exceptions.SSLError as ssl_err:
            logger.error(f"SSLé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {str(ssl_err)}")
            if attempt == max_retries - 1:
                return "âš ï¸ SSLéªŒè¯å¤±è´¥"
            verify_ssl = False
            time.sleep(2 ** attempt)
        except requests.exceptions.RequestException as e:
            logger.error(f"æŠ“å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt == max_retries - 1:
                return "âš ï¸ æ— æ³•æŠ“å–å†…å®¹ï¼ˆå¯èƒ½æ˜¯è®¿é—®é™åˆ¶ï¼‰"
            time.sleep(2 ** attempt)
    return "âš ï¸ æŠ“å–å¤±è´¥: é‡è¯•æ¬¡æ•°è€—å°½"

def universal_search(query: str, num_results: int = 3) -> str:
    """é€šç”¨æœç´¢å¼•æ“æ¥å£ï¼Œä¼˜åŒ–æ•ˆç‡ä»¥è¿”å›æŒ‡å®šæ•°é‡çš„æœ‰æ•ˆç»“æœ"""
    active_apis = [k for k, v in API_CONFIG.items() if v["enabled"]]
    if not active_apis:
        return "âš ï¸ æ— å¯ç”¨æœç´¢å¼•æ“æœåŠ¡"

    max_retries = 2
    for api_type in active_apis:
        config = API_CONFIG[api_type]
        for attempt in range(max_retries):
            try:
                if api_type == "google":
                    params = {
                        "key": config["api_key"],
                        "cx": config["cx"],
                        "q": query,
                        "num": 10,
                        "lr": "lang_zh-CN",
                        "safe": "active"
                    }
                    response = requests.get(config["search_url"], params=params, timeout=15)
                    response.raise_for_status()
                    results = _process_google_results(response.json())

                    valid_results = []
                    for result in results:
                        if len(valid_results) >= num_results:
                            break
                        logger.info(f"æŠ“å–å†…å®¹: {result['link']}")
                        content = fetch_webpage_content(result['link'])
                        if content and not content.startswith("âš ï¸"):
                            result["full_content"] = content
                            valid_results.append(result)

                    if len(valid_results) < num_results:
                        logger.warning(f"ä»…æ‰¾åˆ° {len(valid_results)} ä¸ªæœ‰æ•ˆç»“æœï¼Œæœªè¾¾åˆ° {num_results}")
                    return _format_search_results(valid_results[:num_results], api_type)

            except requests.exceptions.RequestException as e:
                logger.error(f"[{api_type}] è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    API_CONFIG[api_type]["enabled"] = False
                time.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"[{api_type}] æœªçŸ¥é”™è¯¯: {str(e)}")
                break
        if not API_CONFIG[api_type]["enabled"]:
            continue
    return "âš ï¸ æ‰€æœ‰æœç´¢å¼•æ“æœåŠ¡ä¸å¯ç”¨"

def _extract_main_content(soup: BeautifulSoup) -> str:
    """æå–ç½‘é¡µæ­£æ–‡å†…å®¹ï¼Œé¿å…æ— å…³ä¿¡æ¯"""
    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form']):
        tag.decompose()

    possible_selectors = [
        'article', '[role="main"]', '.content', '.article',
        '.post', '.main', '#content', '#main'
    ]
    main_content = None
    for selector in possible_selectors:
        main_content = soup.select_one(selector)
        if main_content:
            break

    if not main_content:
        main_content = soup

    paragraphs = []
    for p in main_content.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
        text = p.get_text().strip()
        if len(text) >= 20 and not any(
            keyword in text.lower() for keyword in ["è·³è½¬", "å¯¼èˆª", "å…³æ³¨", "ç›´æ’­", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ"]
        ):
            paragraphs.append(text)

    return " ".join(paragraphs[:10]) if paragraphs else "æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹"

def _process_google_results(data: dict) -> List[Dict[str, str]]:
    """å¤„ç†Googleæœç´¢ç»“æœå¹¶è¿‡æ»¤æ’é™¤åŸŸå"""
    items = data.get("items", [])
    filtered_items = []
    for item in items:
        link = item.get("link", "")
        if not _is_excluded_url(link):
            filtered_items.append({
                "title": item.get("title", "æ— æ ‡é¢˜"),
                "link": link
            })
    return filtered_items

def _format_search_results(items: List[Dict[str, str]], api_type: str) -> str:
    """æ ¼å¼åŒ–æœç´¢ç»“æœä¸º HTML"""
    if not items:
        return f"âš ï¸ {api_type.capitalize()} æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

    search_summary = f"ğŸ” <b>{api_type.capitalize()}æœç´¢ç»“æœ</b><br><br>"
    for i, item in enumerate(items, 1):
        title = _clean_content(item.get("title", "æ— æ ‡é¢˜"), 80)
        link = item.get("link", "")
        full_content = item.get("full_content", "æœªæŠ“å–å†…å®¹")

        search_summary += (
            f"{i}. <b>{title}</b><br>"
            f"â–¸ å†…å®¹: {full_content[:500]}...<br>"
            f"ğŸŒ <a href=\"{link}\">æ¥æº</a><br><br>"
        )
    return search_summary

def _clean_content(text: str, max_length: int) -> str:
    """æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œç¡®ä¿ç¼–ç å®‰å…¨"""
    text = re.sub(r"å¹¿å‘Š|Sponsored|æ¨è|çƒ­é—¨", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    try:
        text = text.encode('utf-8', 'ignore').decode('utf-8')
    except Exception as e:
        logger.warning(f"æ–‡æœ¬ç¼–ç æ¸…ç†å¤±è´¥: {str(e)}")
        text = text.encode('utf-8', 'replace').decode('utf-8')
    if len(text) > max_length:
        last_valid_index = min(max_length, len(text) - 1)
        for i in range(last_valid_index, 0, -1):
            if ord(text[i]) < 128 or text[i] in "ã€‚ï¼ï¼Ÿï¼›ï¼Œã€":
                last_valid_index = i
                break
        text = text[:last_valid_index] + "..."
    return text

if __name__ == "__main__":
    try:
        while True:
            query = input("è¯·è¾“å…¥æœç´¢å†…å®¹ï¼ˆè¾“å…¥qé€€å‡ºï¼‰: ").strip()
            if query.lower() == "q":
                break
            if not query:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æœç´¢å†…å®¹ï¼")
                continue
            result = universal_search(query)
            print("\n" + result + "\n")
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {str(e)}")